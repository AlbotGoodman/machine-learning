{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allmän linjär regression  \n",
    "\n",
    "[VAB så jag förlitar mig på inspelning - som startade lite sent]  \n",
    "\n",
    "Finn ett hyperplan i p dimensioner ...  \n",
    "\n",
    "$ Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p $  \n",
    "\n",
    "... som optimerar en kostnadsfunktion $\\bold{C(B)}. För OLS används SEE / RSS som kostnadsfunktion:  \n",
    "\n",
    "$ \\bold{C(B)} = SSE $  \n",
    "\n",
    "Beroende på vilken kostnadsfunktion vi väljer så får vi olika regressionsmetoder. Dvs att andra kostnadsfunktioner ger andra regressionsmetoder.  \n",
    "\n",
    "När vi gör en OLS optimerar vi för låg varians. Men det räcker inte alltid så vi måste lägga till mer saker för att minska variansen. \n",
    "\n",
    "**Ridge Regression** (L2-norm):  \n",
    "$ \\bold{C(B)} = MSE + \\sum^p_{i=1} \\beta_i^2 $  \n",
    "\n",
    "**Lasso Regression**  (L1-norm):  \n",
    "$ \\bold{C(B)} = MSE + \\sum^p_{i=1} |\\beta_i| $  \n",
    "\n",
    "När vi gör reguljäriseringsmetoder måste vi skalera/standardisera så att inte de värdena med större skala ses som viktigare än andra.  \n",
    "\n",
    "En regression kostar beräkningsmässigt O(np²) när vi använder SVD - Single Value Decomposition (pseudoinvers).  \n",
    "\n",
    "Vi kan också ändra feature rymden för att minska variansen och få en vettig modell. Ett sätt är att göra våra paramertrar icke-linjära i X så får vi linjarisera förhållandet.  \n",
    "\n",
    "$ Y=BX $  \n",
    "\n",
    "Ovan är ju linjär men X kan ha fler features än variabler, t ex polynomexpansion.  \n",
    "\n",
    "$ Y = x_1 + x_2 + x_1^2 + x_2^2 + x_1x_2 $  \n",
    "\n",
    "Ovan har fem features från två variabler. Vi har två linjära parametrar och två exponentiella och en sadelyta. Absolut inte linjärt i sig om vi hade tänkt i två dimension (eftersom vi har två variabler). Däremot, i fem dimensioner så utgör dessa våra axlar. Och där är ytan Y ett hyperplan och därav linjärt i fem dimensioner. Det går alltid att linjarisera men frågan är om det är en bra approximation eller inte.  \n",
    "\n",
    "Det är beräkningsmässigt väldigt dyrt och dessutom är enklare modeller oftare bättre på att generalisera (Ocham's Razor). Oftast kan polynomexpansion vara smidigt för att hitta samband men oftast vill du sen minska antalet variabler men det kan vara svårt på grund av kolinjaritet. Pga dyr så används den inte ofta eftersom Support Vector Machines gör samma sak utan att expandera feature-rymden. Polynomexpansion är en sorts linjarisering.  \n",
    "\n",
    "$ E[Y|\\bold{X}] = f(\\bold{X}) $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting  \n",
    "\n",
    "**Underfit**  \n",
    "- hög bias  \n",
    "- låg noggrannhet  \n",
    "- stort konfidensintervall  \n",
    "\n",
    "9.2 ± 8.8\n",
    "\n",
    "**Oveerfit**  \n",
    "- låg bias  \n",
    "- hög noggrannhet  \n",
    "- litet konfidensintervall  \n",
    "\n",
    "9.2 ± 0.000131\n",
    "\n",
    "Bias och noggrannhet är varandras inverser.  \n",
    "Varians och precision är varandras inverser.  \n",
    "\n",
    "Overfit är det vanligaste vi råkar ut för och vi vill minska variansen och då kan vi använda reguljäriseringsmetoderna.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent  \n",
    "\n",
    "När vi gör allmän regression så förutsätter vi att vi har en normal-/Gaussisk distribution. Vi kan förstås göra andra antaganden och det mest tydliga:  \n",
    "\n",
    "Om Y ~Bernoulli(p) så har vi ett klassificeringsproblem. Då kan vi inte längre använda OLS (eller ja, det går men inte så bra resultat) och behöver andra sätt att beräkna minimum. Nu för tiden dominerar **GD - Gradient Descent**.  \n",
    "\n",
    "$ \\bold{C(\\theta)} = \\frac{1}{n} \\sum^n_{i=1} (y_i - \\theta X_i)^2 $  \n",
    "\n",
    "Detta är en iterativ process och kan skrivas om såhär:  \n",
    "\n",
    "$ \\theta_{j+1} = \\theta_i - \\eta \\nabla \\bold{C(\\theta)} $  \n",
    "$ \\theta_{j+1} = \\theta_i - \\eta (\\frac{2}{n} \\bold{X^T} (\\bold{X\\theta_j - y})) $  \n",
    "\n",
    "**Batch Gradient Descent**  \n",
    "\n",
    "Beräknas över hela stickprovet. Har en tendens att fastna i lokala minimum.  \n",
    "\n",
    "I ML:  \n",
    "batch = stickprov  (= sample, i statistik)  \n",
    "sample = utfall (=sample point, i statistik)  \n",
    "\n",
    "I ML pratar vi väldigt ofta om batchar och det handlar om batchdimension, hur stora stickprov vi tar åt gången. Vi kommer återkomma till det.  \n",
    "\n",
    "**Stochastic Gradient Descent**  \n",
    "Välj en slumpmässig punkt i varje iteration, räkna gradienten $\\nabla$ på bara den punkten. Upprepa givet antal gånger; detta ger en sk \"epok\". Denna konvergerar inte (når inget optimalt värde) och den slutar aldrig hoppa. Det löser vi med learning rate då vi tar stora steg i början och mindre och mindre.  \n",
    "\n",
    "**Mini batch Gradient Descent**  \n",
    "Välj en slumpmässig delmängd (dvs ett stickprov) och beräkna gradienten $\\nabla$ på det; i övrigt som SGD. Detta är fördelaktigt på GPU/ beräkningshårdvara pga att den kan räkna gradienten på alla punkterna samtidigt eftersom en GPU är bra på parallellprogrammering. Det är största anledningen till denna tekniken för att den går så fort att träna.  \n",
    "\n",
    "**Gradient Descent with momentum**  \n",
    "$ X_{k+1} = X_k - SZ_k $  \n",
    "$ Z_k = \\nabla f_k - \\beta Z_{k-1} $  \n",
    "$ X_{k+1} = X_k - \\eta \\nabla f_k $  \n",
    "\n",
    "Direkt från fysik där:  \n",
    "\n",
    "$ P = MV $  \n",
    "$ E = \\frac{1}{2} MV^2 $  \n",
    "\n",
    "Vad som är speciellt med GD är att det behöver inte finnas några analytiska lösningar. Det enda kravet är att den måste vara differencierbar. Alla är det FÖRUTOM absolutbeloppet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code  \n",
    "\n",
    "Genomgång återigen av L3.  \n",
    "\n",
    "Note: Varför  vi använder .dot är för att vi inte längre har matriser så därav gör vi inte matrismultiplikation utan vi har vektorer och använder skalärprodukt i stället.  \n",
    "\n",
    "En KNN gör en medveten overfit, den lär sig i detalj varje testfall. Det är enda sättet att lära sig så den lär sig egentligen ingenting. Icke-generaliserande. Ett minne som jämför med tidigare exempel. Den kan alltså bara köras på ett data set. Däremot om vi har många fler klasser än tio så blir det svårt för KNN.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

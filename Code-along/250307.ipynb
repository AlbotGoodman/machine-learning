{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassificering\n",
    "\n",
    "Y är nu en punktfördelning, dvs den är diskret, kategorisk och kvalitativ. Om vi har oändligt många klasser har vi en värdesregression.  \n",
    "\n",
    "Vi modellerar sannolikheten att ett värde tillhör en viss klass.  \n",
    "\n",
    "$ P[Y=1 | X] = \\bold{XB} $  \n",
    "\n",
    "Om vi tänker som ovan så blir det någon form av storleksordning vilket är ett problem. Värdena är nödvändigtvis inte i en sannolikhetsfördelning.  \n",
    "\n",
    "Vi måste försöka garantera att få värden mellan 0 och 1. En vanlig funktion är:  \n",
    "\n",
    "$ p(X) = \\frac{e^{\\bold{XB}}}{1 + e^{\\bold{XB}}} $  \n",
    "\n",
    "Detta kallas logistisk funktion. Det är en funktion som är mellan 0 och 1. Uttrycket kan vi manipulera lite grann. \n",
    "\n",
    "$ <=> p(X) + e^{\\bold{XB}} = e^{\\bold{XB}} $  \n",
    "\n",
    "$ <=> \\frac{p(X)}{1 - p(X)} = e^{\\bold{XB}} $  \n",
    "\n",
    "Ovan vänsterled kallar vi för **odds** och går från 0 till oändligheten. Om oddsen är 1/4 så innebär det att 1 av 5 lyckas. Det ser ut såhär:  \n",
    "\n",
    "$ p = 0.2 $  \n",
    "$ \\frac{0.2}{1-0.2} = \\frac{1}{4} $  \n",
    "\n",
    "Ett annat exempel, 9 av 10 lyckas:  \n",
    "\n",
    "$ p = 0.9 $  \n",
    "$ \\frac{0.9}{1-0.9} = 9 $  \n",
    "\n",
    "Det är alltså inte helt uppenbart hur de mappar. Oddsen blir snabbt större när de går uppåt.  \n",
    "\n",
    "$ ln(\\frac{p(X)}{1 - p(X)}) = \\bold{XB} $  \n",
    "\n",
    "Ovan är vad vi kallar för **log it odds**. Maximum likelihood är en generell metod, mer så än OLS. Den försöker hitta en funktion som minimerar hela $\\beta-vektorn$.  \n",
    "\n",
    "$ l(\\beta) = \\Pi_{i = y_i = 1} p(x_i) \\Pi_{i = y = 0} (1 - p(x_i)) $  \n",
    "\n",
    "Vad betyder detta? Vi tar sannolikheten för att vara i [alla 1-klassen] och sannolikheten för att inte vara i den klassen. Vi multiplicera ihop dem och maximerar funktionen. I praktiken använder vi **gradient descent**. Detta är **logistisk regression**.  \n",
    "\n",
    "ln-funktionen är nu vår kostnadsfunktion. Det finns inte nödvändigtvis något linjärt för det finns inneboende kolinjaritet eftersom det vi modellerar på vänstersidan bara är sannolikheten i klasser.  \n",
    "\n",
    "Om vi har många klasser, sk multinomiellt problem, använder vi **softmax** och klassificerar var klass för sig.  \n",
    "\n",
    "Precis som i multipell linjär regression så försöker vi hitta flera linjer även när vi klassificerar. Linjerna är beslutsgränserna, när vi går över dem så hamnar vi i en annan klass. Vi kan använda algoritmer som naive bayes (bara om vi vet grundsanningen), linjär diskriminant analys (LDA), kvadratisk linjär analys (linjerna blir kurvor i stället). I praktiken används inte LDA och KLA så ofta. Det finns bra jämförelse mellan dessa i boken.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vilka metoder har vi?  \n",
    "\n",
    "KNN - k nearest neighbour.  \n",
    "- enkel metod  \n",
    "- icke generaliserande (jämför bara med sitt minne, dvs träningsdatan)  \n",
    "- kan användas både oövervakat och online (går att använda på data utan lablar, de jämför bara med varandra vilket räcker ibland)  \n",
    "- beror på en similarity measure, ofta euklidiskt avstånd men andra exempel är L1 (taxi cab, Manhattan), cos-likhet mm  \n",
    "\n",
    "Linjär regression.  \n",
    "- fungerar men tveksamma resultat.  \n",
    "- använd kanske hellre LDA  \n",
    "\n",
    "Logistisk klassificering  \n",
    "- modellerar sannolikhet i stället för väntevärde  \n",
    "- alltid kolinjaritet  \n",
    "\n",
    "Support Vector Machines\n",
    "- använder kernel trick i stället för feature-expansion  \n",
    "- kan användas i alla möjliga sammanhang, lika användningsbar/kraftfull som ANN  \n",
    "\n",
    "GOFAI (good old fashion AI)  \n",
    "- decision trees, random forest, boosted decision stumps    \n",
    "- klassiska expertmaskiner använde detta, se det ungefär som massa if-satser.  \n",
    "\n",
    "Ensemble learning  \n",
    "- använder olika metoder och låt alla rösta i slutet vilken som ska användas  \n",
    "- används nästan alltid med träd men fungerar ganska bra generellt.  \n",
    "\n",
    "Oövervakad klassificering (clustering)  \n",
    "- använder klustering för att labla data för andra maskininlärningsmetdoder  \n",
    "- k-means ungefär  \n",
    "- PCA, bra för kolinjaritet  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapportskrivning  \n",
    "\n",
    "Det finns färdiga former att använda, IMRAD. Ett visst språk ska användas. Du beskriver inte hur det gick utan alltid dåtid och en utomstående ska förstå utan PM och skriv alltid passivt.  \n",
    "\n",
    "En god rapport är viktigt för VG. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

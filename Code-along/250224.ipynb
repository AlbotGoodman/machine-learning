{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allmän linjär regression\n",
    "\n",
    "Vad vi tittat på hittills är linjär, multipel linjär och polynomial regression.  \n",
    "\n",
    "I allmän linjär regression söker vi linjära approximationer givet ett stickprov (träningsdata). Tänk att vi har någon datamängd med lin approx så hittar vi en derivata (kurvans lutning). När vi når en inflationspunkt så är vår approx inte så bra när den avviker snabbt. Lokalt i små områden är dessa i allmänhet bra approx. Det kallas just lokal regression när vi delar in datan och gör regressioner på mindre bitar av datan. Vanligtvis har vi en väldigt komplex funktion som varierar massvis beroende på omständigheter. Curve fit kallas den också ibland. Väldigt känsliga för varians (overfit) och används oftast inte på grund av det. Står i kapitel 7 i ISLP.  \n",
    "\n",
    "[HÄR PRATAR VI OM VARFÖR VI LÄGGER TILL EN ETTA]  \n",
    "\n",
    "Vad vi vill göra här är att bestämma $\\beta_0, \\beta_1$.  \n",
    "\n",
    "$ min\\bold{C} = \\sum^n_{i=1} (y_i - \\beta_0 - \\beta_iX_i) = \\sum^n_{i=1} (y_i - \\hat{y})^2 $  \n",
    "$ (optimering) \\approx \\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat{y})^2 $  \n",
    "\n",
    "Derivata -> sätt till 0 -> analys  \n",
    "\n",
    "$\\epsilon^2$ står kvar i högerledet, dvs vi optimerar mot låg varians.  \n",
    "\n",
    "Vad vi faktiskt gör är:  \n",
    "\n",
    "$ E[Y|X] = \\bold{XB} $  \n",
    "\n",
    "Vi lär maskinen dessa vikter. Nu har vi bara tittat på en dimension så låt oss utvidga det lite.  \n",
    "\n",
    "Lutningen på ett plan är inte längre bara en skalär utan en gradient. Det skriver vi:  \n",
    "\n",
    "$ (\\frac{d}{dx}, \\frac{d}{dx_2}) = \\nabla f(x_1, x_2) $  \n",
    "\n",
    "Gradienten pekar alltid i den riktning där förändringshastigheten är störst.  \n",
    "\n",
    "$ \\bold{Y} = \\bold{XB} + \\bold{E} $  \n",
    "\n",
    "Om vi gör en feature expansion för att lämna strikt linjära förhållanden. \n",
    "\n",
    "$ Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 $  \n",
    "\n",
    "Vi får någon form av sadelyta. Vi förväntar oss inte längre ett linjärt förhållande.  \n",
    "\n",
    "$ E[Y|X] = f(\\bold{X}) $  \n",
    "\n",
    "En polynomexpansion är en kombinatorisk expansion. Det blir alltså dyrt att köra. Vad gör vi då? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avstickare!  \n",
    "\n",
    "(måltavla)\n",
    "[bild1]  \n",
    "\n",
    "low acc - hög bias  \n",
    "low prec - hög varians  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generell additiv modell (kap 7)\n",
    "\n",
    "$ Y = f_0(x_0) + f_1(x_1) + ... + f_d(x_d) (+ \\epsilon) $  \n",
    "\n",
    "=>  \n",
    "\n",
    "(notera att summatecknen stämmer inte (pga Raphael) men titta på vad som finns inuti)\n",
    "$ \\sum^n_{i=1} f_0(x_0) => \\sum^n_{i=1} x_0 f(\\bold{X}) $  \n",
    "\n",
    "Nu är det sannolikhetsfunktionen (eller?) och f här då bestämmer vilken distribution det är. Genom att välja vilken sannolikhetsfunktion det är så får vi olika distributioner. Nu tittar vi snarare på sannolikheten att Y givet X:  \n",
    "\n",
    "~ Pr(Y|X)  \n",
    "\n",
    "VI ska ha en kostnadsfunktion och en länkfunktion (som kopplar responsen för våra X till sannolikheten i Y). Länkfunktionen för linjär regression är bara medlet (µ). Om vi antar normaldistribution. I ett klassificeringsproblem är det helt enkelt sannolikheten.  \n",
    "\n",
    "Vi kommer att bara titta på logistisk och inte diskriminant (så hoppa det i boken - om vill).  \n",
    "\n",
    "### Reguljärisering  \n",
    "\n",
    "(KOM IHÅG: har du tillräckligt bra resultat behöver du inte reguljärisera!)\n",
    "\n",
    "Vi behöver ta hänsyn till varians. Ridge regression eller reguljäriserar enligt L2-norm.  \n",
    "\n",
    "MSE = Bias² + varians + brus (irreducibel)  \n",
    "\n",
    "Vi kan inte göra något åt bias om vi inte känner till grundsanningen. Därav kan vi inte göra så mycket åt den. Inte bruset heller. Reguljärisering minskar varians men tenderar att öka bias. Det vi vill undvika är:  \n",
    "- underfit (\"för enkel modell\"), låg varians och hög bias (hög precision men låg noggrannhet) pga hög bias så stort konfidensintervall (t ex 9.34526 ± 8.2).   \n",
    "- overfit (\"för komplex modell\"), hög varians och låg bias (låg precision och hög noggrannhet) pga låg bias så litet konfidensintervall (t ex 9.34526 ± 0.00134).   \n",
    "\n",
    "Hur balanserar vi då dessa två? Reguljärisering är att ändra kostnadsfunktionen så att vi straffar lösningen som vi inte vill ha. Eller så gör vi dimensionsreducering och ändrar feature-mängden (PCA).  \n",
    "\n",
    ".\n",
    "\n",
    "Vi har vår kostnadsfunktion (minC(B)) och vår optimeringsmetod för OLS (RSS). Sen lägger vi bara till L2-normen. Vi straffar nu alltså algoritmen att ha stora värden på koefficienterna. Den sprider också ut värdena och fördelningen över alla parametrar och försöker göra var och en så liten som möjligt. Detta är alltså Ridge Regression.    \n",
    "\n",
    "$ min\\bold{C(B)} = RSS + \\lambda \\sum^d_{i=1} \\beta_d^2 $  \n",
    "\n",
    "Däremot om vissa parametrar inte är så signifikanta så är detta inte så bra. Vi kan inte använda statistik och får helt enkelt testa oss fram för att se vad vår MSE blir. Ett annat sätt att göra detta på är vår kostnadsfunktion (minC(B)) och vår optimeringsmetod för OLS (RSS) och lägger till L1-normen (absolutbeloppet) vilket har en speciell effekt. Det kallas Lasso Regression. Denna tenderar att sätta beta-parametrar till noll om de inte är signifikanta.   \n",
    "\n",
    "$ min\\bold{C(B)} = RSS + \\lambda \\sum^d_{i=1} |\\beta_d| $  \n",
    "\n",
    "Lambda ovan är en hyperparameter. Vi ska försöka hitta den lambda som ger oss bäst resultat. Helst skulle vi vilja göra båda dessa, ridge och lasso. Det gör vi såhär:  \n",
    "\n",
    "$ min\\bold{C(B)} = RSS + \\lambda (\\frac{1-\\alpha}{2} \\sum^d_{i=1} \\beta_i^2 + \\alpha \\sum^d_{i=1} |\\beta_i|) $  \n",
    "\n",
    "Här är kostnadsfunktionen för Elastic Net.  \n",
    "\n",
    "När det kommer till klassificering så måste vi standardisera och normalisera. Reguljärisering är inte känsligt för outliers för att få ner variansen. Så kostnadsfunktionen tvingar värdena att bli små för att inte outliers ska påverka lika mycket.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3-Regularization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature standardisation\n",
    "\n",
    "En ridge regression tenderar att fördela över alla parametrar för att göra de mindre. Men om parametrarna är på olika skalor får vi problem. Därför måste vi standardisera datan först. Annars blir det att parametrar av störst skala har störst påverkan.  \n",
    "\n",
    "Vi måste behålla vår scaler senare eftersom den måste användas hela igenom.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression  \n",
    "\n",
    "Notera att sklearn använder annan terminologi än dagens lektion. De byter plats på alpha och lambda.  \n",
    "\n",
    "Som vi ser så får vi 19 features från grunddatans 3 st och vi vet sedan tidigare att \"Newspaper\" inte är relevant. Så när vi sprider ut ... över alla parametrar så blir det inte så bra. Vårt resultat är inte så bra. Vi får testa med något annat!  \n",
    "\n",
    "Om vi tar lasso nu och skriver ut våra parametrar så ser vi att den har eliminerat några parametrar (satt dem till noll). Sen går det att testa olika alpha i detta fall men för att låta datorn göra det så kör vi k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation \n",
    "\n",
    "(inte säker på att nedan blev helt korrekt)\n",
    "Tar tränings set, slumpar, delar upp i grupper, för varje använder vi testdata. Sen börjar vi om, tar en grupp, tränar på alla och tar medlet från våra RMSE, MSE osv under alla körningar och sen när vi hittat optimala så kan vi säga att den ska variera våra hyperparametrar. Sist när vi har en bra hyperparam så tränar vi om på hela datan igen och då måste vi använda en faktisk, riktig, testdata eftersom vi använde övriga som validering.  \n",
    "\n",
    "I sklearn så har de vänt på det så att höga tal är bättre i stället för våra vanliga låga värden som vi önskar.  \n",
    "\n",
    "Här märker vi också att värdena blir sämre än en multipel linjär regression. Ofta finns det bättre sätt än att använda polynomexpansion, speciellt med tillgång till SVM (support vector machines).  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

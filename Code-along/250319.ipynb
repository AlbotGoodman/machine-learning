{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA kan hjälpa när vi har korrelationer. I labben kan vi kombinera höjd och vikt till ett gemensamt värde för att få bort korrelation, det är ett sätt att göra det på.  \n",
    "\n",
    "Oövervakad teknik. Huvudsakliga med PCa är otrogonaliseringen - att hitta saker som är oberoende av varandra. Hitta okorrelerade variabler som förklarar så mycket att variansen som möjligt. Matematiskt gör vi detta med linjärkombinartioner.  \n",
    "\n",
    "Eigenvector (egenvektor) är en vektor som blir densamma när den transformeras, inte bara nollvektorn. Med de kan vi lista ut massa saker med matriserna. Nu letar vi efter egenvektorerna till kovariansmatrisen - de är en sorts grundlösningar. Om vi använder egenvektorerna som baser [...]. Vi ser dessa som principalkomponenter.  \n",
    "\n",
    "Hur många komponenter/dimensioner ska vi välja? Det är vår hyperparameter som behöver optimeras. Enormt viktigt att vi standardiserar! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här tar vi lika många komponenter som originalet, vi letar bara efter ortogonalitet. Detta är feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.figure(), plt.axes()\n",
    "\n",
    "ax.plot(range(1, len(proportion_variance_explained)+1),\n",
    "        proportion_variance_explained, 'o--')\n",
    "\n",
    "ax.set(title=\"Proportion variance explained (PVE) by principal components\",\n",
    "       xlabel=\"Number of principal components\", ylabel=\"PVE\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vill komma så nära 1 som möjligt men komma så billigt undan. Enligt grafen kanske 10 st komponenter är ett bra värde. Möjligtvis 15 st. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Våra komponenter ser ut att vara linjärt separerbara. Vi vet att de är ortagonala. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vad en PCA faktiskt gör är att den hittar de bredaste riktningarna och förskjuter dem så att de blir rätvinkliga (ortogonala). Tänk ett kluster där den hittar den största variansen, två punkter för ett kluster som är längst ifrån varandra. Sen hittar den två punkter till som har stort avstånd till varandra. Sen gör den dessa rätvinkliga mot varandra. Finns det flera kluster och vi kör på hela datan så gör den det fortfarande på punkterna längst ifrån varandra, nu blir det tvärs över flera kluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ord är intressanta. Termfrekvens (relativ frekvens av en term i ett dokument): förekomsten av ett ord / alla ord.  \n",
    "\n",
    "Sentiment analysis, är textmassan positiv eller negativ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [text.lower().split() for text in [review1, review2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det finns ju information i att viss text är skriven i versaler exempelvis men numeriskt så har de ingen koppling (UTF-8 värdena t ex). Därför konverterar vi allt till gemener innan vi senare omvandlar till numerisk form för att datorn ska kunna läsa.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om ovan var TF så är IDF antalet dokument med ordet / antalet dokument. TF-IDF är sedan dessa två resultat multiplicerat med varandra.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den snabbaste och bästa metoden för att vektorisera text. I LLM så tittar de på sekvenser av ord också t ex. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

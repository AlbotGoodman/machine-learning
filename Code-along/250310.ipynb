{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "missade början"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avstånd punkt/plan \n",
    "\n",
    "Jag vill först säga något om hyperplan som kan vara bra att veta.  \n",
    "\n",
    "$ P_0 = (x_0, y_0, z_0) $  \n",
    "$ A = (p, q, r) $  \n",
    "$ |OP| = \\frac{| (X_0 - p)a + (y_0 - q)b + (z_0 - r)c |}{\\sqrt{a^2 + b^2 + x^2}} $  \n",
    "$ \\frac{| \\bold{u \\cdot n} |}{\\bold{n \\cdot n}} | \\bold{n} | $  \n",
    "$ <=> \\bold{n \\cdot n} = | \\bold{n} | ^2 $  \n",
    "\n",
    "När längden är 1, dvs normalen är en enhetsvektor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Margin:  \n",
    "\n",
    "$ \\sum^r_{j=1} \\beta^2_j = 1 $  \n",
    "\n",
    "Med så få punkter som möjligt försöker vi maximera marginalen. Om marginalen skulle vara tom på punkter så blir metoden väldigt känslig för brus. Då kan vi använda Soft Margin.  \n",
    "\n",
    "$ \\beta_0 + \\beta_pX_{ip} + ... + \\beta_pX_{ip} \\geq M (1) $  \n",
    "\n",
    "[hann inte med]  \n",
    "\n",
    "## Kernel\n",
    "\n",
    "En kernel är en matris med vikter:  \n",
    "\n",
    "$ \\bold{u_j} \\cdot \\bold{v_j} = \\sum^p_{j=1} u_j v_j $  \n",
    "\n",
    "$\\bold{u \\cdot v}$ kallas även inre produkt och skrivs $<\\bold{u, v}>$.  \n",
    "\n",
    "$ \\beta_0 + \\sum_{i \\epsilon S} \\hat{\\alpha_i} <x, x_i> $  \n",
    "\n",
    "S är en mänhd av stödvektorer. Vad vi kan göra med denna är en polynomiell feature expansion med hjälp av stödvektorerna när vi räknar våran maximering. Tricket för att det inte ska vara lika dyrt som poly är stödvektorerna. Vikollapsar alla andra dimensioner där dessa stödvektorerna inte finns (eller är noll). De flesta alpha_hat blir noll så vi struntar i alla punkter förutom de som är skiljda från noll.  \n",
    "\n",
    "Sen använder vi då en kernelfunktion.  \n",
    "\n",
    "$ K (\\bold{u, v}) = (1 + \\sum^p_{j=1} u_j v_j)^d $  \n",
    "\n",
    "Ovan beräknar inre produkter för d-dimensionella polynom. Vi kan se det som att den har:  \n",
    "\n",
    "$ (^{p + d}_d) $  \n",
    "\n",
    "I praktiken använder vi oftast något som heter RBF - radial basis function.  \n",
    "\n",
    "$ K (\\bold{u, v}) = e^{-\\gamma \\sum^p_{i=1} (\\bold{u_i - v_i})^2} $  \n",
    "\n",
    "I ovan är $\\gamma$ en hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Använd .to_numpy() för att få fram arrays från dataframes.  \n",
    "\n",
    "\"Längsta avståndet till de närmsta punkterna\", max margin.  \n",
    "\"Minsta avståndet till alla punkterna\", OLS.  \n",
    "\n",
    "Soft margin tillåter några felklassificeringar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om datan däremot inte är linjär så får vi använda kernel trick.  \n",
    "\n",
    "Om vi sätter C till 10 så är det i stort sett en max margin classifier. Reguljärisering någonting? När C blir mindre blir den dyrare men bättre när fler punkter kommer innanför vår margin. Lågt C = hög varians, högt C = hög bias.  \n",
    "\n",
    "I alla fall där statistiska metoder inte räcker till så är SVM inte bra. Nu när ANN tagit över så används inte så många klassiska tekniker förutom logistisk regression, beslutsträd och SVM. SVM är ett alternativ sätt att göra de andra teknikerna. T ex när vi använder en sigmoid kernel så är det o stort sett logistisk regression. Skillnaden är att vi använder en bättre optimerare (soft margin) och i stället för att ändra vår feature space så ... .  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

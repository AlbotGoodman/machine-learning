{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisering\n",
    "\n",
    "Om vi lägger ihop bruset och de är stora mot skalan på datan så är regularisering bra. Du kan ha så mycket brus att det döljer dina features, du får massa olika resultat och det kan se ut som en underfit. Den är inte för enkel men kan inte anpassa pga brus. Komplexa modeller i hög dimension är bra att regularisera på grund av att det blir långa avstånd. Om det då är en avvikelse så blir den överdriven. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Vi representerar datan som en iterativ funktion. Det är en klass av matematiska objekt. I allmänhet terminerar de aldrig. De måste nå en fixpunkt och det är långt ifrån alla ekvationer som har det. Deterministiskt kaos.  \n",
    "\n",
    "SGD är att vi tar gradienten på ett slumpmässigt steg, använder den för att ta ett litet steg, så håller vi på så.  \n",
    "\n",
    "MBGD så tar vi ett random batch.  \n",
    "\n",
    "Learning rate gångrar vi in i varje steg så att stegen blir mindre och mindre. Men vi måste sätta ett stoppvillkor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassificering\n",
    "\n",
    "I vårt exempel tar vi bilder på siffror och gör om de till en vektor i stället för en matris. Här vill vi gärna skalera (standardisering).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Vi behöver kunna räkna ut accuracy, precision, recall, f1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "Vi lär oss ingenting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "\n",
    "Behöver en parameterlista vi HPO men kan också användas för att testa mot okänd data. Scoring i detta fallet styr till viss del hur optimering går till, vad vi använder som mått. I värdesregression är det ju MSE men här är det accuracy normalt sett. I scoring går till och med att skicka med en egen funktion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Maximum Margin Classifier. OLS hittar minsta avståndet till alla punkter i ett dataset. Men denna metid vänder på det, avståndet ska vara så långt som möjligt till vissa punkter (de närmsta punkterna till grundsanningen) - då kan vi uppnå samma sak.  \n",
    "\n",
    "Maximeringsproblemet är att summan av kvadraterna av våra parametrar ska vara ett. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "Egentligen en stor uppsättning av if/else-satser som beslutas om det är sant eller falskt. Lite som flödesscheman. Användes historiskt i expertsystem.  \n",
    "\n",
    "Skalera inte datan.  \n",
    "\n",
    "Större träd är känsligare för varians och förändringar. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

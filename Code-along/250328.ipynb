{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse vs Blessing of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curse: i många dimensioner blir avstånden större och dyrt att beräkna  \n",
    "Blessing: vi kan linjärisera väldigt icke-linjära förhållanden i högre dimensioner  \n",
    "Små modeller med hög R2 eller låg MSE är att föredra, då de generaliserar bättre (dvs bättre på okänd data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Två sätt att välja feature mängd, särskilt inom linjära ... :  \n",
    "- forward selection\n",
    "- backward elimination\n",
    "\n",
    "Mer vanligt om du utvecklar från början. I din vanliga roll får du given data och vilka parametrar som ska användas.   \n",
    "\n",
    "Forward:  \n",
    "- kör p-st enkla regressioner på varje parameter och väljer den med högst förklaringsgrad (R2)  \n",
    "- testa restrerande p-1 variabler, lägg till en åt gången och se om R2 ökar/minskar \n",
    "- problem med kolinjäritet så kan vi inte längre lita på statistiken (jämförelser med R2), inte säkert att det ökar och minskar som vi tror.  \n",
    "\n",
    "Backward:  \n",
    "- börja med alla p-variabler, utvärdera\n",
    "- testa alla modeller med p-1 variabler (gå alltså baklänges)\n",
    "- fortsätt tills R2 blir sämre\n",
    "- särskilt jänslig för mulikolinjäritet (som dessutom är svårt att upptäcka)\n",
    "\n",
    "Ett annat sätt att hantera dimensionsreducering är PCA. Både en dimensionsreduceringsteknik och hanterar multikolinjäritet. Vad är grundidén? Det är att hitta en ortogonalbas till designmatrisen (dvs hitta en minimal oberoende uppsättning variabler).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X = \\Phi \\bold{B} $  \n",
    "\n",
    "Där B är en basbytesmatris. Det finn singa exakta lösningar, använd statistik. Skriv upp varje variabel $X_i$ som en linjärkombination av Z:  \n",
    "\n",
    "$ Z_i = \\phi_{1 i} X_1 + \\phi_{2 i} X_2 + ... \\phi_{p i} X_p $  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "$ Z_n = \\phi_{1 n} X_1 + \\phi_{2 n} X_2 + ... \\phi_{p n} X_p $  \n",
    "\n",
    "Välj $\\phi_{ij}$ så att variansen är maximal!  \n",
    "\n",
    "Optimeringsproblemet ser ut såhär (en kostnadsfunktion):  \n",
    "\n",
    "$ max [\\frac{1}{n} \\sum^n_{i=1} (\\sum^p_{j=1} \\phi_{ij} X_i)^2] $  \n",
    "$ bivillkor \\sum^p_{j=1} \\phi{ji}^2 = 1 $  \n",
    "\n",
    "Här måste det vara standard-normal (standardisera och normalisera).  \n",
    "\n",
    "Med PCA lär vi oss ingenting men det är fortfarande en maskininlärningsmetod eftersom det är en kodning av kunskapen från feature-mängden. Oövervakad inlärning - \"vi lär oss systemkolinjäritet\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "Ett annat sätt att göra en dimensionsreducering är K-Means Clustering. Grundidén är att minimera intraklustervariation (ett kluster ska vara så tätt som möjligt).  \n",
    "- hitta mängder där punkterna är så nära varandra som möjligt\n",
    "- räknar ut en centroid (räknar ut mitten)\n",
    "\n",
    "Optimeringsproblem:  \n",
    "\n",
    "$ (^{min}_{C_1 ... C_k}) [\\sum^k_{k=1} \\frac{1}{|C_k|} \\sum_{i,i' \\epsilon C_k} \\sum^p_{j=1} (X_{ij} - X_{i'j})^2] $  \n",
    "\n",
    "Hitta minimum till varje klass/kluster genom att räkna ut medlet och för varje par räknar vi ihop kombinationer och räknar ut medel. $|C_k|$ är antalet punkter i $C_k$-klustern. Framför allt används det för att oövervakat labla.  \n",
    "\n",
    "Hur minskar detta dimensioner? När vi har alla punkter kan vi ersätta dem med sina medel. Då minskar vi antalet datapunkter (antingen rader eller variabler). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konstnadsfunktioner\n",
    "\n",
    "I allmänhet är våra maskininlärningsproblem ett optimeringsproblem. Det vi optimerar är en kostnadsfunktion.  \n",
    "\n",
    "$ \\bold{C(B)} = \\sum^n_{i=1} (y_i - \\hat{y})^2 $  (RSS)  \n",
    "\n",
    "$ \\bold{C(B)} = \\frac{1}{n}\\sum_{i=1} (y_i - ŷ)^2 $  (MSE)  \n",
    "\n",
    "MSE = bias² + varians + brus  \n",
    "\n",
    "Om bias dominerar över bruset (den riktiga variansen så att säga) men vi har låg varians då är vi i en **underfit**.  \n",
    "\n",
    "Minska varians med hjälp av reguljärisering:  \n",
    "\n",
    "$ \\bold{C(B)} = MSE + \\lambda \\sum^p_{i=1} \\beta_i^2 $ (Ridge regression, l2-norm)  \n",
    "\n",
    "$\\lambda$ är en hyperparameter. Tänk bX - om b är liten så ger det ett litet utslag när X ändras. Denna reguljärisering tenderar att sprida ut fördelningen över alla $\\beta$ oavsett deras signifikans.  \n",
    "\n",
    "$ \\bold{C(B)} = MSE + \\lambda \\sum^p_{i=1} |\\beta_i| $ (Lasso regression, l1-norm)  \n",
    "\n",
    "Denna tenderar att sätta vissa parametrar till noll. Om $\\lambda$ är för stort hamnar vi snabbt i underfit pga den sätter för mycket noll. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

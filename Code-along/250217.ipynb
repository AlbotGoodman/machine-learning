{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matematik - analys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) - en funktion med \"x\" som parameter, dvs f(x) - x² är i allmänhet inte en funktion. \n",
    "\n",
    "T ex $\\pm$ 1 dvs ej enkelvärd. \n",
    "[bild1]  \n",
    "\n",
    "Om vi begränsar oss till bara + eller  - så är [HANN INTE]\n",
    "\n",
    "x² + y² = 1  \n",
    "f(x, y) = x² + y² -1  \n",
    "\n",
    "Nu är det två parametrar och därav en funktion. Detta kallas nu allmän flervariabelanalys men i vårt fall begränsar vi oss till linjära. Vi skulle kunna lägga till två parametrar till här.  \n",
    "\n",
    "f(x,y,z,w) = xz + yw -1  \n",
    "x = z  \n",
    "y = w  \n",
    "Då får vi:  \n",
    "x^ + y² - 1  \n",
    "\n",
    "Det vi behöver kunna från analys är deivering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivering  \n",
    "\n",
    "I två dimensioner handlar det om att hitta lutningen på linjer. Vi minskar dimensionen med ett när vi deriverar och vi gör det linjärt.  \n",
    "\n",
    "y = kx + m  \n",
    "\n",
    "Sätt att skriva derivata:  \n",
    "\n",
    "$ \\dot{y} = y' = \\frac{d}{dx}y $  \n",
    "\n",
    "$ \\frac{d}{dx}ax^n = n * a * x^{n-1} $  \n",
    "$ \\frac{d}{dx}y = kx⁰ + 0 = k $  \n",
    "\n",
    "[bild2] a -> 0  \n",
    "envariabel (en dimension)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrering (primitiv funktion)  \n",
    "\n",
    "Det omvända från derivering.  \n",
    "\n",
    "$ \\int x² blir \\frac{x³}{3} + c $  \n",
    "\n",
    "När vi deriverar så är det linjärt men när vi går åt andra hållet så är det en affin. Det kommer alltid finnas en XXXXX konstant. Det kommer ställa till det för oss när vi gör backprobagation men det tar vi sen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivata i flera dimensioner  \n",
    "\n",
    "$ f(x_1, x_2, ..., x_n) $  \n",
    "\n",
    "Varje x är ju en (dimensions-) axel.  \n",
    "\n",
    "[bild3]  sfär i fyra dimensioner  \n",
    "\n",
    "En derivata pekar nu åt olika håll när vi har fler parametrar. Detta kallas gradient (flervariabelderivata) men är under ytan inte längre en vanlig derivata utan snarare en differential. Hur ser den ut då?  \n",
    "\n",
    "Skrivs som en triangel. $ \\nabla $  \n",
    "\n",
    "$ \\nabla f(x,y) = (\\frac{d}{dx}, \\frac{d}{dx})$  [OSÄKER PÅ OM DETTA BLEV RÄTT, SKA VARA SNIRKLIGT D]  \n",
    "\n",
    "Lutningen blir nu vektorer i flera dimensioner.  \n",
    "\n",
    "I ML vill vi hitta ett hyperplan som skiljer, en beslutsgräns. Tänk en 3D-graf där det finns massa cirklar eller kryss och vi vill då att planet ska separerara dessa två.  \n",
    "\n",
    "Gradienter löser vi med något som kallas *stochastic gradient descent* (SGD). Den gör att vi slumpmässigt går nedåt längs ... . Detta är ett alternativ till OLS för att hitta linjen.  \n",
    "\n",
    "[bild4]  \n",
    "\n",
    "Den tar slumpmässiga steg nedåt med högre sannolikhet att vi tar steg i rätt riktning och men det kan också gå uppåt. Oftast tittar vi på värdet som den antar och när stegen börjar bli mindre så slutar vi. Det finns alltså inget stoppvillkor. Varför gör vi detta? OLS kan ju fastna i ett lokalt minimum. Det finns alltså ingen garanti för att vi hittar det verkliga minimum. Om vi använder adaptive SGD (t ex Adam - Adaptive Moment Estimation) så tar den stora hopp först och sen mindre och mindre.   \n",
    "\n",
    "[bild5]  \n",
    "\n",
    "Det är väldigt sällan som det går att återskapa samma resultat när vi använder denna metoden.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

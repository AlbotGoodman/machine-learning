{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Exercises/E01_gradient_descent.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; to see hints and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient descent exercises\n",
    "\n",
    "---\n",
    "These are introductory exercises in Machine learning with focus in **gradient descent** .\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> all datasets used in this exercise can be found under Data folder of the course Github repo</p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that in cases when you start to repeat code, try not to. Create functions to reuse code instead. </p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Remember</b> to use <b>descriptive variable, function, index </b> and <b> column names</b> in order to get readable code </p>\n",
    "\n",
    "The number of stars (\\*), (\\*\\*), (\\*\\*\\*) denotes the difficulty level of the task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simulate dataset (*)\n",
    "\n",
    "Simulate datasets according to these rules:\n",
    "\n",
    "- set random seed to 42\n",
    "- (1000,2) samples from $X \\sim \\mathcal{U}(0,1)$ , i.e. 1000 rows, 2 columns. \n",
    "- 1000 samples from $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "- $y = 3x_1 + 5x_2 + 3 + \\epsilon$ , where $x_i$ is column $i$ of $X$\n",
    "\n",
    "Finally add a column of ones for the intercept to $X$.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use for simulating X\n",
    "\n",
    "´´´\n",
    "np.random.rand(samples, 2)\n",
    "´´´\n",
    "\n",
    "to concatenate with ones, use ```np.c_[..., ...]```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "```\n",
    "array([[1.        , 0.37454012, 0.95071431],\n",
    "       [1.        , 0.73199394, 0.59865848],\n",
    "       [1.        , 0.15601864, 0.15599452],\n",
    "       [1.        , 0.05808361, 0.86617615],\n",
    "       [1.        , 0.60111501, 0.70807258]])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 2)\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + 3 + np.random.randn(1000)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, '.'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent - learning rate (*)\n",
    "\n",
    "Use gradient descent to calculate $\\vec{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^T$ \n",
    "\n",
    "&nbsp; a) Use $\\eta = 0.1$ and calculate $\\vec{\\theta}$ for each fifth epoch from 1 to 500. So the procedure is as follows:\n",
    "- calculate $\\vec{\\theta}$ for epochs = 1\n",
    "- calculate $\\vec{\\theta}$ for epochs = 6\n",
    "- ...\n",
    "- calculate $\\vec{\\theta}$ for epochs = 496\n",
    "\n",
    "Plot these $\\vec{\\theta}$ values against epochs. (*)\n",
    "\n",
    "&nbsp; b) Do the same as for a) but with learning rate $\\eta = 0.01$, 5000 epochs and for each 20th epoch. What do you notice when changing the learning rate? (*)\n",
    "\n",
    "&nbsp; c) Experiment with larger and smaller $\\eta$ and see what happens.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "a) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg.png\" height=\"200\"/>\n",
    "\n",
    "b) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg_001.png\" height=\"200\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD5(X, y, lr=0.1):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n)\n",
    "    epochs = range(1, 500, 5)\n",
    "    theta_list = []\n",
    "    \n",
    "    for _ in epochs:\n",
    "        gradient = (2 / m) * X.T @ ((X @ theta)- y)\n",
    "        theta -= lr * gradient\n",
    "        theta_list.append(theta)\n",
    "\n",
    "    theta_list = np.array(theta_list)\n",
    "    \n",
    "    return theta, epochs, theta_list\n",
    "\n",
    "\n",
    "def GD20(X, y, lr=0.01):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n)\n",
    "    epochs = range(1, 5000, 20)\n",
    "    theta_list = []\n",
    "    \n",
    "    for _ in epochs:\n",
    "        gradient = (2 / m) * X.T @ ((X @ theta)- y)\n",
    "        theta -= lr * gradient\n",
    "        theta_list.append(theta)\n",
    "    \n",
    "    theta_list = np.array(theta_list)\n",
    "    \n",
    "    return theta, epochs, theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta5, epochs5, theta5_list = GD5(X_train, y_train)\n",
    "theta20, epochs20, theta20_list = GD20(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs5, theta5_list, label=[\"theta0\", \"theta1\", \"theta2\"])\n",
    "ax.legend()\n",
    "ax.set_title('Learning rate = 0.1\\n', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta20_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent - learning rate (**)\n",
    "\n",
    "Repeat task 1 but using stochastic gradient descent instead. Also adjust number of epochs to see if you can find convergence. What kind of conclusions can you draw from your experiments. (**)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini Batch Gradient Descent (**)\n",
    "\n",
    "Now try different sizes of mini-batches and make some exploratory plots to see convergence. Also you can make comparison to the other algorithms by using same $\\eta$ and same amount of epochs to see how they differ from each other in terms of convergence. (**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

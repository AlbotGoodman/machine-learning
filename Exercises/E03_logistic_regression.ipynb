{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Exercises/E04_logistic_regression.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; to see hints and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic regression exercises \n",
    "\n",
    "---\n",
    "These are introductory exercises in Machine learning with focus in **logistic regression**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that sometimes you don't get exactly the same answer as I get, but it doesn't neccessarily mean it is wrong. Could be some parameters, randomization, that we have different. Also very important is that in the future there won't be any answer sheets, use your skills in data analysis, mathematics and statistics to back up your work.</p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that in cases when you start to repeat code, try not to. Create functions to reuse code instead. </p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Remember</b> to use <b>descriptive variable, function, index </b> and <b> column names</b> in order to get readable code </p>\n",
    "\n",
    "The number of stars (\\*), (\\*\\*), (\\*\\*\\*) denotes the difficulty level of the task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Iris flower dataset (*)\n",
    "\n",
    "In the whole exercise, we will work with the famous Iris flower dataset, which was collected in 1936 by Ronald Fisher, a statistician and biologist. Use the ```datasets``` module from scikit-learn to load the iris dataset. \n",
    "\n",
    "&nbsp; a) Check keys on the loaded data and check what the different values for each key are.\n",
    "\n",
    "&nbsp; b) Now insert the data into a DataFrame. \n",
    "\n",
    "&nbsp; c) Do some EDA to get an understanding of the dataset. \n",
    "\n",
    "&nbsp; d) Make a correlation heatmap to see how each feature is correlated to each other. What do the numbers mean?\n",
    "\n",
    "&nbsp; e) Make a boxplot. The points outside of the boxplot are statistically calculated outliers using Tukey's rule for boxplot. \n",
    "\n",
    "&nbsp; f) Now remove the outliers in data. (**)\n",
    "\n",
    "- Lower bound outlier: $Q_1 - 1.5\\cdot IQR$\n",
    "- Upper bound outlier: $Q_3 + 1.5\\cdot IQR$\n",
    "\n",
    "where $Q_1$ is the 1st quartile or 25 percentile, $Q_3$ is the 3rd quartile or 75 percentile and $IQR = Q_3-Q_1$ is the interquartile range. \n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint</summary>\n",
    "\n",
    "a) For DESCR key you need to print it.\n",
    "\n",
    "f) Dataframes has a quantile method.  \n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "b) \n",
    "\n",
    "|    |   sepal length (cm) |   sepal width (cm) |   petal length (cm) |   petal width (cm) |   species | specie_name   |\n",
    "|---:|--------------------:|-------------------:|--------------------:|-------------------:|----------:|:--------------|\n",
    "|  0 |                 5.1 |                3.5 |                 1.4 |                0.2 |         0 | setosa        |\n",
    "|  1 |                 4.9 |                3   |                 1.4 |                0.2 |         0 | setosa        |\n",
    "|  2 |                 4.7 |                3.2 |                 1.3 |                0.2 |         0 | setosa        |\n",
    "|  3 |                 4.6 |                3.1 |                 1.5 |                0.2 |         0 | setosa        |\n",
    "|  4 |                 5   |                3.6 |                 1.4 |                0.2 |         0 | setosa        |\n",
    "\n",
    "c) When you do describe, remove species as its statistical values are meaningless. \n",
    "\n",
    "|                   |    mean |      std |   min |   25% |   50% |   75% |   max |\n",
    "|:------------------|--------:|---------:|------:|------:|------:|------:|------:|\n",
    "| sepal length (cm) | 5.84333 | 0.828066 |   4.3 |   5.1 |  5.8  |   6.4 |   7.9 |\n",
    "| sepal width (cm)  | 3.05733 | 0.435866 |   2   |   2.8 |  3    |   3.3 |   4.4 |\n",
    "| petal length (cm) | 3.758   | 1.7653   |   1   |   1.6 |  4.35 |   5.1 |   6.9 |\n",
    "| petal width (cm)  | 1.19933 | 0.762238 |   0.1 |   0.3 |  1.3  |   1.8 |   2.5 |\n",
    "\n",
    "\n",
    "<img src = \"../assets/pairplot_iris.png\" height=300>\n",
    "\n",
    "Do more EDA than I show here. \n",
    "\n",
    "d) Correlation heatmap\n",
    "\n",
    "<img src = \"../assets/Correlation_iris.png\" height=300>\n",
    "\n",
    "The closer the value is to 1 between two features, the more positively linear relationships between them. The closer the value is to -1 the more negatively linear relationships between them. \n",
    "\n",
    "e) \n",
    "\n",
    "<img src = \"../assets/boxplot_iris.png\" height=200>\n",
    "\n",
    "f)\n",
    "Outlier rows are: [13, 15, 22, 23, 24, 41, 43, 44, 98, 106, 117, 119, 131]\n",
    "\n",
    "value counts:\n",
    "\n",
    "|            |   specie_name |\n",
    "|:-----------|--------------:|\n",
    "| versicolor |            49 |\n",
    "| virginica  |            46 |\n",
    "| setosa     |            42 |\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "data = pd.DataFrame(iris.data, columns=feature_names)\n",
    "df = data.copy()\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, target_names)\n",
    "# print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe_T = df.describe().transpose().reset_index()\n",
    "df_describe_T.drop(columns=[\"count\"], inplace=True)\n",
    "df_describe_T_melt = df_describe_T.melt(id_vars=\"index\")\n",
    "\n",
    "sns.barplot(data=df_describe_T_melt, x=\"value\", y=\"variable\", hue=\"index\")\n",
    "plt.title(\"iris_dataset.describe() visualised\\n\", fontweight=\"bold\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"x\")\n",
    "plt.gca().set_facecolor('whitesmoke')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_numeric = df.describe().columns.to_list()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 5))\n",
    "\n",
    "for ax, col in zip(axes, cols_numeric):\n",
    "    sns.boxplot(data=df, y=col, ax=ax, hue=\"species\")\n",
    "    ax.set_title(f'Boxplot of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "sns.pairplot(df, hue = \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quantile(df, flower):\n",
    "\n",
    "    df_flower = df[df[\"species\"] == flower]\n",
    "    df_flower = df_flower.drop(columns=\"species\")\n",
    "\n",
    "    q1 = df_flower.quantile(0.25)\n",
    "    q3 = df_flower.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    col_list = df_flower.columns.to_list()\n",
    "\n",
    "    for col in col_list:\n",
    "        df_flower = df_flower[(df_flower[col] >= lower[col]) & (df_flower[col] <= upper[col])]\n",
    "    \n",
    "    df_flower[\"species\"] = flower\n",
    "\n",
    "    return df_flower\n",
    "\n",
    "\n",
    "species_list = df[\"species\"].unique().tolist()\n",
    "\n",
    "df_cleaned = pd.DataFrame()\n",
    "for flower in species_list:\n",
    "    df_mask_cleaned = find_quantile(df, flower)\n",
    "    df_cleaned = pd.concat([df_cleaned, df_mask_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quantile_test(df, flower):\n",
    "    df_flower = df[df[\"species\"] == flower].drop(columns=\"species\")\n",
    "    q1 = df_flower.quantile(0.25)\n",
    "    q3 = df_flower.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    for col in df_flower.columns:\n",
    "        df_flower = df_flower[(df_flower[col] >= lower[col]) & (df_flower[col] <= upper[col])]\n",
    "    df_flower[\"species\"] = flower\n",
    "    return df_flower\n",
    "\n",
    "species_list = df[\"species\"].unique().tolist()\n",
    "df_cleaned_test = pd.concat([find_quantile_test(df, flower) for flower in species_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split and scale data (*)\n",
    "\n",
    "Do train|test split and scale the data using feature standardization, I used default test size 0.33 and random state 42. Check the mean and standard deviation on training and test data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.drop(columns = \"species\")\n",
    "y = df_cleaned[\"species\"]\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_scaled_values = pd.DataFrame({\n",
    "    \"mean\": scaler.mean_,\n",
    "    \"std\": np.sqrt(scaler.var_)\n",
    "}, index=feature_names).T\n",
    "\n",
    "df_scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify with logistic regression (*)\n",
    "\n",
    "Use k-folded cross-validation with logistic regression to find suitable hyperparameters and model. Check the documentation to see which parameters that can be chosen through cross-validation. Check the models parameters and see what it has chosen. \n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "weights: \n",
    "\n",
    "```py\n",
    "array([[-1.33033256,  1.35076961, -2.26169407, -2.07715072],\n",
    "       [ 0.40073538, -0.28598722, -0.58388865, -0.7782766 ],\n",
    "       [ 0.67977172, -0.81485664,  3.09503329,  3.10542664]])\n",
    "```\n",
    "\n",
    "$\\ell_1$-ratio:\n",
    "\n",
    "```py\n",
    "array([0.2, 0.2, 0.2])\n",
    "```\n",
    "\n",
    "\n",
    "<img src = \"../assets/pairplot_iris.png\" height=300>\n",
    "\n",
    "Do more EDA than I show here. \n",
    "\n",
    "d) Correlation heatmap\n",
    "\n",
    "<img src = \"../assets/Correlation_iris.png\" height=300>\n",
    "\n",
    "The closer the value is to 1 between two features, the more positively linear relationships between them. The closer the value is to -1 the more negatively linear relationships between them. \n",
    "\n",
    "e) \n",
    "\n",
    "<img src = \"../assets/boxplot_iris.png\" height=200>\n",
    "\n",
    "f)\n",
    "Outlier rows are: [13, 15, 22, 23, 24, 41, 43, 44, 98, 106, 117, 119, 131]\n",
    "\n",
    "value counts:\n",
    "\n",
    "|            |   specie_name |\n",
    "|:-----------|--------------:|\n",
    "| versicolor |            49 |\n",
    "| virginica  |            46 |\n",
    "| setosa     |            42 |\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionCV(cv=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coefficients: \\n{model.coef_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate model (*)\n",
    "\n",
    "Make a prediction on the testing data. \n",
    "\n",
    "&nbsp; a) Check manually the first 10 values of $y_{test}$ against your prediction. \n",
    "\n",
    "&nbsp; b) Plot a confusion matrix. Can you see which predictions the model have mispredicted?\n",
    "\n",
    "&nbsp; c) Print a classification report \n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "\n",
    "b) \n",
    "\n",
    "<img src = \"../assets/confusion_matrix_iris.png\" height=300>\n",
    "\n",
    "\n",
    "\n",
    "c) \n",
    "\n",
    "Classification report \n",
    "\n",
    "```py\n",
    "          precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        14\n",
    "           1       1.00      0.94      0.97        16\n",
    "           2       0.94      1.00      0.97        16\n",
    "\n",
    "    accuracy                           0.98        46\n",
    "   macro avg       0.98      0.98      0.98        46\n",
    "weighted avg       0.98      0.98      0.98        46\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "cm_train = confusion_matrix(y_train, y_pred_train, labels = model.classes_)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test, labels = model.classes_)\n",
    "disp_train = ConfusionMatrixDisplay(cm_train, display_labels = model.classes_)\n",
    "disp_test = ConfusionMatrixDisplay(cm_test, display_labels = model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subplot 1x2\n",
    "\n",
    "disp_train.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {acc_train}\\nTesting accuracy: {acc_test}\\n\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. $k$-folded cross-validation for evaluation (**)\n",
    "\n",
    "To be more robust in reporting the results, you should report the results as $\\mu_{score}$, i.e. average score through a k-folded cross-validation. Report the score for precision, recall, f1-score for each label and overall accuracy. Do the cross-validation manually using for statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(y_true, y_pred, split, cv):\n",
    "\n",
    "    pre = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    rec = recall_score(y_true, y_pred, average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "        \"Precision\": pre, \n",
    "        \"Recall\": rec, \n",
    "        \"F1-score\": f1, \n",
    "        \"Accuracy\": acc, \n",
    "        \"CV\": cv},\n",
    "        index=pd.Index([split], name=\"Split\")).reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_predict_score(X_train, X_test, y_train, cv=5):\n",
    "    \n",
    "    model = LogisticRegressionCV(cv=cv)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    df_train = calculate_scores(y_train, y_pred_train, \"train\", cv)\n",
    "    df_test = calculate_scores(y_test, y_pred_test, \"test\", cv)\n",
    "\n",
    "    return pd.concat([df_train, df_test])\n",
    "\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "for k in range(2, 11):\n",
    "    scores_cv = fit_predict_score(X_train_scaled, X_test_scaled, y_train, cv=k)\n",
    "    scores = pd.concat([scores, scores_cv])\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=scores, x=\"CV\", y=???, hue=\"Split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
